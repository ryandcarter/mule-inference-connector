<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<mule xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation" xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core" xmlns:mac-inference="http://www.mulesoft.org/schema/mule/mac-inference" xmlns:munit="http://www.mulesoft.org/schema/mule/munit" xmlns:munit-tools="http://www.mulesoft.org/schema/mule/munit-tools" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="         http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd         http://www.mulesoft.org/schema/mule/munit http://www.mulesoft.org/schema/mule/munit/current/mule-munit.xsd         http://www.mulesoft.org/schema/mule/munit-tools http://www.mulesoft.org/schema/mule/munit-tools/current/mule-munit-tools.xsd         http://www.mulesoft.org/schema/mule/mac-inference http://www.mulesoft.org/schema/mule/mac-inference/current/mule-mac-inference.xsd         http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd">

	<!-- This test is used to test the LLM capability to answer questions that are not part of provided functions.
	Note: Not all LLMs have this capability and therefore not all inference types will be covered as part of this test
	-->
	<munit:config name="tools-native-template-llm-response.xml">
		<munit:parameterizations>
			<munit:parameterization name="config-ai21labs" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="Ai21labsConfig" />
					<munit:parameter propertyName="llmModel" value="${ai21labs.llmModel}" />
					<munit:parameter propertyName="inputCount" value="478" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-anthropic" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="AnthropicConfig" />
					<munit:parameter propertyName="llmModel" value="${anthropic.llmModel}" />
					<munit:parameter propertyName="inputCount" value="671" />
					<munit:parameter propertyName="finishReason" value="end_turn" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-azure-ai-foundry" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="azureAiFoundryConfig" />
					<munit:parameter propertyName="llmModel" value="${azure-ai-foundry.llmModel}" />
					<munit:parameter propertyName="inputCount" value="423" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-azure-openai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="AzureOpenAIConfig" />
					<munit:parameter propertyName="llmModel" value="${azure-openai.llmModel}" />
					<munit:parameter propertyName="inputCount" value="198" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-cerebras" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="CerebrasConfig" />
					<munit:parameter propertyName="llmModel" value="${cerebras.llmModel}" />
					<munit:parameter propertyName="inputCount" value="493" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-cohere" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="CohereConfig" />
					<munit:parameter propertyName="llmModel" value="${cohere.llmModel}" />
					<munit:parameter propertyName="inputCount" value="150" />
					<munit:parameter propertyName="finishReason" value="COMPLETE" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-deepinfra" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="DeepinfraConfig" />
					<munit:parameter propertyName="llmModel" value="${deepinfra.llmModel}" />
					<munit:parameter propertyName="inputCount" value="640" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<!-- Deepseek integrations might not work in local due to sever restrictions, but works with jenkins pipeline-->
			<munit:parameterization name="config-deepseek" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="DeepseekConfig" />
					<munit:parameter propertyName="llmModel" value="${deepseek.llmModel}" />
					<munit:parameter propertyName="inputCount" value="441" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-github" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="GithubConfig" />
					<munit:parameter propertyName="llmModel" value="${github.llmModel}" />
					<munit:parameter propertyName="inputCount" value="198" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-groq" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="GroqConfig" />
					<munit:parameter propertyName="llmModel" value="${groq.llmModel}" />
					<munit:parameter propertyName="inputCount" value="491" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-hugging-face" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="HuggingFaceConfig" />
					<munit:parameter propertyName="llmModel" value="${hugging-face.llmModel}" />
					<munit:parameter propertyName="inputCount" value="24" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-mistralai">
				<munit:parameters>
					<munit:parameter propertyName="config" value="MistralAIConfig"/>
					<munit:parameter propertyName="llmModel" value="${mistral.llmModel}"/>
					<munit:parameter propertyName="inputCount" value="339"/>
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-nvidia">
				<munit:parameters>
					<munit:parameter propertyName="config" value="NvidiaConfig"/>
					<munit:parameter propertyName="llmModel" value="${nvidia.llmModel}"/>
					<munit:parameter propertyName="inputCount" value="567"/>
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openai">
				<munit:parameters>
					<munit:parameter propertyName="config" value="OpenAIConfig"/>
					<munit:parameter propertyName="llmModel" value="${openai.llmModel}"/>
					<munit:parameter propertyName="inputCount" value="198"/>
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openai-compatible">
				<munit:parameters>
					<munit:parameter propertyName="config" value="OpenAICompatibleConfig"/>
					<munit:parameter propertyName="llmModel" value="${openai-compatible.llmModel}"/>
					<munit:parameter propertyName="inputCount" value="198"/>
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openrouter" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OpenrouterConfig" />
					<munit:parameter propertyName="llmModel" value="${openrouter.llmModel}" />
					<munit:parameter propertyName="inputCount" value="337" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-portkey" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="PortkeyConfig" />
					<munit:parameter propertyName="llmModel" value="${portkey.llmModel}" />
					<munit:parameter propertyName="inputCount" value="339" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-xinference" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="XInferenceConfig" />
					<munit:parameter propertyName="llmModel" value="${xinference.modelResponse}" />
					<munit:parameter propertyName="inputCount" value="442" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-zhipu" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="ZhipuConfig" />
					<munit:parameter propertyName="llmModel" value="${zhipu.llmModel}" />
					<munit:parameter propertyName="inputCount" value="439" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
		</munit:parameterizations>
	</munit:config>

	<munit:test description="Test" doc:id="da368b58-d280-4c01-975a-f443d8038e53" name="TOOLS_NATIVE_TEMPLATE_OPERATION-General-Test">
		<munit:execution>
			<flow-ref doc:id="857e9da2-0d1d-45ca-bd2f-3909efff3841" doc:name="Flow-ref to TOOLS_NATIVE_TEMPLATE_OPERATION" name="tools_native_template_general"/>
			<logger doc:id="afa15bb1-3ba6-430d-b3ed-81103128695d" doc:name="Logger" level="INFO" message="#[payload]"/>
		</munit:execution>
		<munit:validation>
			<!-- Fixed: Use payload.payload.tools -->
			<munit-tools:assert-that doc:id="034ef6d8-64a0-4be0-b441-7d80c059f748" doc:name="Tools must be empty" expression="#[(sizeOf(payload.payload.tools default []) == 0)]" is="#[MunitTools::equalTo(true)]" message="Tools must be empty"/>
			<!-- Fixed: Use payload.payload.tools -->
			<munit-tools:assert-that doc:id="cb23f789-4c28-446c-9ee1-343c443d1636" doc:name="Answer Check" expression="#[payload.payload.response]" is="#[MunitTools::containsString('New Delhi')]" message="The answer is not correct, it should be New Delhi"/>
			<!-- Fixed: Use payload.payload.tools -->
			<munit-tools:assert-equals actual="#[attributes.tokenUsage.inputCount as String]" doc:id="8a3d65aa-b548-44cf-a627-82c7d0580659" doc:name="Input Token" expected="${inputCount}" message="Incorrect Input Token"/>
			<munit-tools:assert-that
					doc:name="Model Info"
					doc:id="f688a025-3466-4afe-bacb-6a2275258f96"
					message="Incorrect Model Info"
					expression="#[attributes.additionalAttributes.model]"
					is="#[MunitTools::containsString('${llmModel}')]" />
			<munit-tools:assert-equals actual="#[attributes.additionalAttributes.finishReason]" doc:id="f9f30053-da19-4052-91b8-690a24c03948" doc:name="finish reason" expected="${finishReason}" message="Incorrect finish reason"/>
		</munit:validation>
	</munit:test>

	<sub-flow doc:id="71465f76-1597-41ec-bcb7-3d4aa45c3cac" name="tools_native_template_general">
		<set-variable doc:id="7edeecec-4d47-4bfe-a699-23987b60ac14" doc:name="Set Variable" value="#[%dw 2.0
            output application/json
            ---
            {
                'template': 'You are an helpful assistant',
                'instructions': 'Answer the request with politeness.',
                'dataset': 'What is the capital of India',
                'tools': [
                    {
                        'type': 'function',
                        'function': {
                            'name': 'get_current_temperature',
                            'description': 'Get the current temperature for a specific location',
                            'parameters': {
                                'type': 'object',
                                'properties': {
                                    'location': {
                                        'type': 'string',
                                        'description': 'The city and state, e.g., San Francisco, CA'
                                    },
                                    'unit': {
                                        'type': 'string',
                                        'enum': ['Celsius', 'Fahrenheit'],
                                        'description': 'The temperature unit to use. Infer this from the user\'s location.'&#10;                                    }&#10;                                },&#10;                                'required': ['location', 'unit']&#10;                            }&#10;                        }&#10;                    },&#10;                    {&#10;                        'type': 'function',&#10;                        'function': {&#10;                            'name': 'get_rain_probability',&#10;                            'description': 'Get the probability of rain for a specific location',&#10;                            'parameters': {&#10;                                'type': 'object',&#10;                                'properties': {&#10;                                    'location': {&#10;                                        'type': 'string',&#10;                                        'description': 'The city and state, e.g., San Francisco, CA'&#10;                                    }&#10;                                },&#10;                                'required': ['location']&#10;                            }&#10;                        }&#10;                    },&#10;                    {&#10;                        'type': 'function',&#10;                        'function': {&#10;                            'name': 'get_delivery_date',&#10;                            'description': 'Get the delivery date for a customer\'s order. Call this whenever you need to know the delivery date, for example when a customer asks \'Where is my package\'',
                            'parameters': {
                                'type': 'object',
                                'properties': {
                                    'order_id': {
                                        'type': 'string',
                                        'description': 'The customer\'s order ID.'&#10;                                    }&#10;                                },&#10;                                'required': ['order_id']&#10;                            }&#10;                        }&#10;                    }&#10;                ]&#10;            }]" variableName="testPayload"/>
		<mac-inference:tools-native-template config-ref="${config}" doc:id="c98bbd6c-69b5-4334-a1db-ade5aa25897d" doc:name="[Tools] Native Template (Reasoning only)">
			<mac-inference:template><![CDATA[#[vars.testPayload.template]]]></mac-inference:template>
			<mac-inference:instructions><![CDATA[#[vars.testPayload.instructions]]]></mac-inference:instructions>
			<mac-inference:data><![CDATA[#[vars.testPayload.dataset]]]></mac-inference:data>
			<mac-inference:tools><![CDATA[#[vars.testPayload.tools]]]></mac-inference:tools>
		</mac-inference:tools-native-template>
		<ee:transform doc:id="8eda33bd-db2f-412a-b448-dc633d326a39" doc:name="Transform Message">
			<ee:message>
				<ee:set-payload><![CDATA[%dw 2.0
                output application/json
                ---
                {
                    payload: payload,
                    attributes: attributes
                }]]></ee:set-payload>
			</ee:message>
		</ee:transform>
	</sub-flow>
</mule>